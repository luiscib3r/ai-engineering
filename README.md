# AI Engineering con LLMs en Rust

> Serie educativa sobre implementaci√≥n real de sistemas LLM desde los fundamentos hasta producci√≥n

[![Rust](https://img.shields.io/badge/rust-2024-orange.svg)](https://www.rust-lang.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-work%20in%20progress-yellow.svg)]()

## ‚ö†Ô∏è Work in Progress

Este proyecto est√° en desarrollo activo. El roadmap puede cambiar seg√∫n disponibilidad de tiempo e intereses personales o de la comunidad que decida contribuir. **Pull Requests son bienvenidos** para expandir la serie con nuevos conceptos o mejorar implementaciones existentes.

## üéØ Sobre este Proyecto

Esta es una **serie de art√≠culos t√©cnicos** (no un curso formal) que explora la implementaci√≥n de sistemas con Large Language Models desde una perspectiva pragm√°tica y sin hype. El objetivo es entender y controlar cada capa del stack: desde tokenizaci√≥n hasta serving, pasando por inferencia, logits, sampling y RAG.

**Enfoque anti-hype**: Nada de "conecta LangChain y haz magia". Aqu√≠ construimos desde los fundamentos, entendiendo c√≥mo funcionan realmente los LLMs y tomando control sobre cada componente del sistema.

**Por qu√© Rust**: Control de bajo nivel, rendimiento, safety y concurrencia. Ideal para entender los detalles de implementaci√≥n sin sacrificar productividad.

## üìö Contenido de la Serie

### 1. **Tokens** - Los Fundamentos
Entendiendo la tokenizaci√≥n: c√≥mo el texto se convierte en n√∫meros que un modelo puede procesar.

**Conceptos clave**:
- Tokenizaci√≥n con HuggingFace Tokenizers
- Vocabulario y encoding
- Manejo de caracteres especiales

**Ejecutar**:
```bash
make tokens
```

### 2. **Logits** - Entendiendo la Salida del Modelo
An√°lisis profundo de logits, probabilidades y sampling strategies.

**Conceptos clave**:
- Forward pass y generaci√≥n de logits
- Conversi√≥n de logits a probabilidades (softmax)
- Estrategias de sampling (greedy, temperature-based)
- Carga y uso de modelos cuantizados (GGUF)
- Aceleraci√≥n por hardware (Metal, CUDA)

**Ejecutar**:
```bash
make logits
```

### 3. **Haiku** - Generaci√≥n de Texto Completa
Implementaci√≥n end-to-end de un generador de texto con diferentes configuraciones.

**Conceptos clave**:
- Generaci√≥n autoregresiva
- Control de temperatura y creatividad
- Manejo de tokens especiales (EOS)
- Streaming de output
- Comparaci√≥n de estrategias de generaci√≥n

**Ejecutar**:
```bash
make haiku
```

## üöÄ Quick Start

### Requisitos

- Rust 1.85+ (Edition 2024)
- Cargo
- Conexi√≥n a internet (para descargar modelos de HuggingFace)

### Instalaci√≥n

```bash
# Clonar el repositorio
git clone https://github.com/tu-usuario/ai-engineering-rust.git
cd ai-engineering-rust

# Compilar todos los proyectos
cargo build --release

# Ejecutar cualquier ejemplo
make tokens
make logits
make haiku
```

### Configuraci√≥n

El archivo `config.yaml` define los modelos y tokenizers utilizados:

```yaml
tokenizer:
  repo: "Qwen/Qwen3-4B"
  file: "tokenizer.json"

llm:
  repo: "unsloth/Qwen3-4B-GGUF"
  file: "Qwen3-4B-Q4_K_M.gguf"
  branch: "main"
```

Los modelos se descargan autom√°ticamente desde HuggingFace Hub en la primera ejecuci√≥n.

## üèóÔ∏è Arquitectura del Proyecto

Este es un **workspace de Cargo** con m√∫ltiples crates independientes:

```
ai-engineering-rust/
‚îú‚îÄ‚îÄ tokens/          # Fundamentos de tokenizaci√≥n
‚îú‚îÄ‚îÄ logits/          # An√°lisis de logits y sampling
‚îú‚îÄ‚îÄ haiku/           # Generaci√≥n de texto completa
‚îú‚îÄ‚îÄ config.yaml      # Configuraci√≥n de modelos
‚îî‚îÄ‚îÄ Cargo.toml       # Workspace configuration
```

### Stack Tecnol√≥gico

- **[Candle](https://github.com/huggingface/candle)**: Framework de ML en Rust (HuggingFace)
- **[Tokenizers](https://github.com/huggingface/tokenizers)**: Tokenizaci√≥n r√°pida
- **[hf-hub](https://github.com/huggingface/hf-hub)**: Cliente para HuggingFace Hub
- **[GGUF](https://github.com/ggerganov/ggml)**: Formato de modelos cuantizados

### Aceleraci√≥n por Hardware

El proyecto soporta m√∫ltiples backends de aceleraci√≥n:

- **Metal**: Para GPUs de Apple Silicon
- **CUDA**: Para GPUs NVIDIA
- **Accelerate**: Framework de Apple para optimizaci√≥n en CPU
- **MKL**: Intel Math Kernel Library
- **CPU**: Fallback sin aceleraci√≥n

## üéì Conceptos Explorados

### Tokenizaci√≥n
- Byte-Pair Encoding (BPE)
- Vocabulario y mapeo token-id
- Encoding y decoding
- Tokens especiales y control

### Inferencia
- Forward pass en transformers
- Carga de modelos cuantizados (Q4_K_M)
- Optimizaci√≥n de memoria con GGUF
- Detecci√≥n y uso de aceleradores

### Logits y Probabilidades
- Raw logits vs probabilidades
- Softmax transformation
- Top-k analysis
- Interpretaci√≥n de scores

### Sampling Strategies
- **Greedy**: Siempre el token m√°s probable (temp ‚âà 0)
- **Balanced**: Temperature moderada (0.7)
- **Creative**: Alta temperatura (1.5)
- **Chaos**: Temperatura muy alta (2.0)

### Generaci√≥n Autoregresiva
- Loop de generaci√≥n token-by-token
- Manejo de contexto posicional
- Early stopping con EOS tokens
- Streaming de output

## üîß Comandos √ötiles

```bash
# Compilar todo el workspace
cargo build --release

# Ejecutar con features espec√≠ficos (ejemplo: Metal en macOS)
cargo run --bin logits --features metal,accelerate

# Limpiar builds
cargo clean

# Verificar dependencias
cargo tree

# Ejecutar con logs detallados
RUST_LOG=debug make haiku
```

## üìñ Filosof√≠a del Proyecto

### Pragmatismo sobre Hype
En lugar de usar abstracciones de alto nivel que ocultan la complejidad, este proyecto:
- Expone los detalles de implementaci√≥n
- Explica el "por qu√©" de cada decisi√≥n t√©cnica
- Muestra trade-offs reales (velocidad vs calidad, memoria vs precisi√≥n)
- No asume que "m√°s complejo = mejor"

### Control Real
- Acceso directo a logits pre-softmax
- Implementaci√≥n custom de sampling
- Manipulaci√≥n expl√≠cita de tensores
- Sin capas de abstracci√≥n innecesarias

### Aprendizaje Profundo
No es suficiente con "hacer que funcione". El objetivo es:
- Entender cada componente del pipeline
- Poder debuggear problemas reales
- Tomar decisiones informadas sobre arquitectura
- Construir intuici√≥n sobre el comportamiento de los LLMs

## üõ£Ô∏è Roadmap

> **Nota**: Este roadmap es flexible y puede cambiar seg√∫n disponibilidad de tiempo e intereses de la comunidad.

- [x] Tokenizaci√≥n b√°sica
- [x] Inferencia y an√°lisis de logits
- [x] Generaci√≥n de texto con sampling
- [ ] Implementaci√≥n de RAG (Retrieval-Augmented Generation)
- [ ] Embeddings y b√∫squeda sem√°ntica
- [ ] Serving con API REST
- [ ] Streaming de respuestas con SSE
- [ ] Fine-tuning con LoRA
- [ ] Evaluaci√≥n y benchmarking

¬øTienes ideas para expandir la serie? **¬°Los Pull Requests son bienvenidos!**

## ü§ù Contribuciones

Este es un proyecto educativo abierto y **los Pull Requests son bienvenidos**. Como este es un trabajo en progreso que evoluciona seg√∫n disponibilidad de tiempo e intereses de la comunidad, tu participaci√≥n puede ayudar a expandir y mejorar la serie.

### C√≥mo contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/amazing-feature`)
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

### Gu√≠as de Contribuci√≥n
- Mant√©n el enfoque pragm√°tico y educativo
- Documenta decisiones t√©cnicas y el "por qu√©"
- Incluye ejemplos ejecutables y reproducibles
- Evita abstracciones innecesarias
- Si a√±ades un nuevo m√≥dulo, actualiza el README y el Makefile

### Ideas de Contribuci√≥n
- Nuevos ejemplos explorando conceptos espec√≠ficos
- Optimizaciones de rendimiento
- Soporte para nuevos modelos o arquitecturas
- Mejoras en documentaci√≥n y explicaciones
- Herramientas de visualizaci√≥n o debugging

## üôè Agradecimientos

- [HuggingFace](https://huggingface.co/) por Candle y los modelos
- [Qwen Team](https://github.com/QwenLM) por Qwen3
- [unsloth](https://huggingface.co/unsloth) por las versiones GGUF optimizadas
- La comunidad de Rust por herramientas excepcionales

## üìö Recursos Adicionales

- [Candle Examples](https://github.com/huggingface/candle/tree/main/candle-examples)
- [Tokenizers Docs](https://huggingface.co/docs/tokenizers/index)
- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (Paper original de Transformers)

---

**Construido con ü¶Ä Rust y ‚ù§Ô∏è por Luis Correa**
