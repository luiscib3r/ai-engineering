# AI Engineering con LLMs en Rust

> Serie educativa sobre implementaci√≥n de sistemas LLM desde los fundamentos hasta producci√≥n

[![Rust](https://img.shields.io/badge/rust-2024-orange.svg)](https://www.rust-lang.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-work%20in%20progress-yellow.svg)]()

## üéØ Sobre este Proyecto

Serie de art√≠culos t√©cnicos que explora la implementaci√≥n de sistemas con Large Language Models desde una perspectiva pragm√°tica. El objetivo es entender y controlar cada capa del stack: desde tokenizaci√≥n hasta serving, pasando por inferencia, logits, sampling y RAG.

**Enfoque**: Sin abstracciones m√°gicas. Construimos desde los fundamentos, entendiendo c√≥mo funcionan realmente los LLMs.

**Por qu√© Rust**: Control de bajo nivel, rendimiento, safety y concurrencia.

## üìù Art√≠culos

Serie de art√≠culos t√©cnicos que explican en profundidad los conceptos implementados en este repositorio:

1. [**Qu√© Pasa Cuando un LLM "Piensa": Tokens, Logits, y Sampling**](https://www.luisciber.com/p/que-pasa-cuando-un-llm-piensa-tokens)  
   Explicaci√≥n completa del proceso interno de inferencia en LLMs: desde la tokenizaci√≥n del texto hasta la generaci√≥n de respuestas, pasando por logits, probabilidades y estrategias de sampling.

## üìö Contenido de la Serie

### 1. **Tokens** - Los Fundamentos
Entendiendo c√≥mo el texto se convierte en n√∫meros que un modelo puede procesar.

**Conceptos**: Tokenizaci√≥n con HuggingFace, vocabulario, encoding/decoding, caracteres especiales.

```bash
make tokens
```

### 2. **Logits** - Entendiendo la Salida del Modelo
An√°lisis de logits, probabilidades y estrategias de sampling.

**Conceptos**: Forward pass, logits a probabilidades (softmax), estrategias de sampling (greedy, temperature), modelos cuantizados (GGUF), aceleraci√≥n por hardware.

```bash
make logits
```

### 3. **Haiku** - Generaci√≥n de Texto Completa
Implementaci√≥n end-to-end de un generador de texto con diferentes configuraciones.

**Conceptos**: Generaci√≥n autoregresiva, control de temperatura, tokens especiales (EOS), streaming, comparaci√≥n de estrategias.

```bash
make haiku
```

### 4. **LLM Inference** - Pipeline Completo de Inferencia
Implementaci√≥n profesional del pipeline completo: prefill, KV cache, y loop autoregresivo.

**Conceptos**: Prefill optimizado, KV cache, generaci√≥n autoregresiva eficiente, chat templates, configuraci√≥n avanzada de sampling (top-k, top-p), estad√≠sticas de generaci√≥n.

```bash
make llm-inference
```

## üöÄ Quick Start

```bash
# Clonar el repositorio
git clone https://github.com/luisciber/ai-engineering-rust.git
cd ai-engineering-rust

# Compilar todos los proyectos
cargo build --release

# Ejecutar ejemplos
make tokens
make logits
make haiku
make llm-inference
```

### Configuraci√≥n

El archivo `config.yaml` define los modelos y par√°metros de inferencia:

```yaml
tokenizer:
  repo: "Qwen/Qwen3-4B"
  file: "tokenizer.json"

llm:
  repo: "unsloth/Qwen3-4B-GGUF"
  file: "Qwen3-4B-Q4_K_M.gguf"

inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  max_length: 256
```

## üèóÔ∏è Arquitectura

Workspace de Cargo con m√∫ltiples crates independientes:

```
ai-engineering-rust/
‚îú‚îÄ‚îÄ tokens/          # Tokenizaci√≥n b√°sica
‚îú‚îÄ‚îÄ logits/          # An√°lisis de logits y sampling
‚îú‚îÄ‚îÄ haiku/           # Generaci√≥n de texto
‚îú‚îÄ‚îÄ llm-inference/   # Pipeline completo de inferencia
‚îú‚îÄ‚îÄ config.yaml      # Configuraci√≥n de modelos
‚îî‚îÄ‚îÄ Cargo.toml       # Workspace configuration
```

### Stack Tecnol√≥gico

- **[Candle](https://github.com/huggingface/candle)**: Framework de ML en Rust (HuggingFace)
- **[Tokenizers](https://github.com/huggingface/tokenizers)**: Tokenizaci√≥n r√°pida
- **[hf-hub](https://github.com/huggingface/hf-hub)**: Cliente para HuggingFace Hub
- **[GGUF](https://github.com/ggerganov/ggml)**: Formato de modelos cuantizados

### Aceleraci√≥n por Hardware

Soporta m√∫ltiples backends: Metal (Apple Silicon), CUDA (NVIDIA), Accelerate, MKL, CPU.

## üéì Conceptos Clave

- **Tokenizaci√≥n**: BPE, vocabulario, encoding/decoding
- **Inferencia**: Forward pass, modelos cuantizados, optimizaci√≥n de memoria
- **Logits**: Raw logits vs probabilidades, softmax, top-k analysis
- **Sampling**: Greedy, temperature-based, top-k, top-p
- **Generaci√≥n**: Loop autoregresivo, KV cache, prefill, streaming, EOS tokens

## üõ£Ô∏è Roadmap

- [x] Tokenizaci√≥n b√°sica
- [x] Inferencia y an√°lisis de logits
- [x] Generaci√≥n de texto con sampling
- [x] Pipeline completo de inferencia
- [ ] Implementaci√≥n de RAG
- [ ] Embeddings y b√∫squeda sem√°ntica
- [ ] Serving con API REST
- [ ] Fine-tuning con LoRA

## ü§ù Contribuciones

Los Pull Requests son bienvenidos para expandir la serie con nuevos conceptos o mejorar implementaciones.

### C√≥mo contribuir

1. Fork el repositorio
2. Crea una rama (`git checkout -b feature/amazing-feature`)
3. Commit tus cambios (`git commit -m 'Add amazing feature'`)
4. Push a la rama (`git push origin feature/amazing-feature`)
5. Abre un Pull Request

### Gu√≠as

- Mant√©n el enfoque pragm√°tico y educativo
- Documenta decisiones t√©cnicas y el "por qu√©"
- Incluye ejemplos ejecutables
- Evita abstracciones innecesarias

## üôè Agradecimientos

- [HuggingFace](https://huggingface.co/) por Candle y los modelos
- [Qwen Team](https://github.com/QwenLM) por Qwen3
- [unsloth](https://huggingface.co/unsloth) por las versiones GGUF optimizadas

## üìö Recursos

- [Candle Examples](https://github.com/huggingface/candle/tree/main/candle-examples)
- [Tokenizers Docs](https://huggingface.co/docs/tokenizers/index)
- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)

---

**Construido con ü¶Ä Rust y ‚ù§Ô∏è por [Luis Correa](https://www.luisciber.com)**